---
title: "Homework 2 - Untidy data"
output: html_document
author:
- Yuli Song
- John Chang
- Chen Tao
---

## Setup - Load packages

```{r setup, include = FALSE}
# For task2, we will use ggplot2 package
install.packages("ggplot2")
install.packages("dplyr")
install.packages("tidyr")

library(dplyr)
library(tidyr)
library(ggplot2)
```

## Task 1 - Lego Sales Data

```{r message = FALSE}
sales = readRDS("data/lego_sales.rds")
```

### Part 1 - Tidying the data


#### Write up

First we convert the lego dataset into tibble format and unnest each row. As the element "purchases" is a list of lists of elements and these elements contain some useful information, we unnest "purchases" twice. For other columns, we do not need to unnest here. Please note, to deal with duplicate purchases, we choose to code the purchase as a single row that includes the variable "Quantity".

#### Code

```{r task1_tidy}
# Covert the `sales` object into a tidy data frame
sales_df = tibble::tibble(
  people = sales
)

sales_df = unnest_wider(sales_df, people) %>%
  unnest_longer(purchases) %>%
  unnest_wider(purchases)

# check NA in columns
colSums(is.na(sales_df))
```

```{r task1_top}
# Print out the first 10 lines of the tidy data
print(sales_df %>% slice(1:10))
```


### Part 2 - Questions

#### Question 1

```{r task1_q1}
sales_df %>%
  select(first_name, last_name, phone_number) %>%
  distinct() %>%
  count(first_name) %>%
  top_n(n = 3) %>%
  arrange(desc(n))
```

#### Question 2

```{r task1_q2}
sales_df %>%
  select(Theme, USPrice, Quantity) %>%
  group_by(Theme) %>%
  summarise(Money = sum(USPrice*Quantity), .groups = 'drop') %>%
  top_n(n = 1)
```


#### Question 3

```{r task1_q3}
# Here we need to sum up each person's purchasing quantity of different toy sets first, and then calculate the purchasing amount per person for the female and the male.
sales_df %>%
  select(gender, first_name, last_name, phone_number, Quantity) %>%
  group_by(first_name, last_name, phone_number) %>%
  summarise(gender, total = sum(Quantity), .groups = "drop") %>%
  distinct() %>%
  group_by(gender) %>%
  summarise(average = sum(total)/n(), .groups = "drop")

# From the output, we can find that the female bought more Lego sets.
```

#### Question 4

```{r task1_q4}
# Here we need to enumerate each person's hobbies first, and then count the numbers of different kinds of hobbies. Finally, we output and arrange the five most popular hobbies of Lego purchasers.
sales_df %>%
  select(first_name, last_name, phone_number, hobbies) %>%
  distinct() %>%
  select(hobbies) %>%
  unnest_longer(hobbies) %>%
  filter(is.na(hobbies) == FALSE) %>%
  count(hobbies) %>%
  top_n(n = 5) %>%
  arrange(desc(n))
```

#### Question 5

```{r task1_q5}
sales_df %>%
  select(phone_number, USPrice, Quantity) %>%
  filter(is.na(phone_number) == FALSE) %>%
  mutate(area_name = substr(phone_number, 1, 3)) %>%
  transmute(area_name, USPrice, Quantity) %>%
  group_by(area_name) %>%
  summarise(total_price = sum(USPrice*Quantity), .groups = "drop") %>%
  top_n(n = 1)

# From the output, we can see that the "956" area code has spent the most money (719.96$) on Legos.
# Note: Inside the dataset, some elements inside "phone_number" column is NA. As NA means that the area code is unknown, we will ignore those purchasing history for this question.
```



## Task 2 - GitHub and dplyr

```{r message = FALSE}
commits = readRDS("data/dplyr_commits.rds")
commits = tibble::tibble(
    steps = commits
)

# Here we select one single column in the dataset and then unnest it to check the structure of the dataset.
unnest_wider(commits, steps)

unnest_wider(commits, steps) %>%
  select(commit) %>%
  unnest_wider(commit, names_repair = "unique")

unnest_wider(commits, steps) %>%
  select(author) %>%
  unnest_wider(author, names_repair = "unique")

unnest_wider(commits, steps) %>%
  select(committer) %>%
  unnest_wider(committer, names_repair = "unique")

unnest_wider(commits, steps) %>%
    select(parents) %>%
    unnest_longer(parents, names_repair = "unique") %>%
    unnest_wider(parents, names_repair = "unique") 

unnest_wider(commits, steps) %>%
  select(stats) %>%
  unnest_wider(stats, names_repair = "unique")

unnest_wider(commits, steps) %>%
  select(files) %>%
  unnest_longer(files, names_repair = "unique") %>%
  unnest_wider(files, names_repair = "unique")

```

### Part 1 - Structure of a commit


Each commit contains the variables "sha", "node_id", "commits", "url", "html_url", "author", "committer", "parents", "stats" and "files" columns. The "commits", "author", "committer", "parents", "stats" and "files" are lists of lists and the elements in other columns are not lists. The elements inside each list is shown above in detail.


### Part 2 - Tidying the data


#### Write up

For this section, we have again converted the data into tibble type. After we unnest the data, we then select the columns that are useful for the questions, namely, the author's name, date and time of each commit, and the files column. The reason why we do not unnest the files column is because it will cause us to over count the authors for question 1. We also notice that certain names were stored in different formats, so we have replaced and combined them.


#### Code

```{r task2_tidy}
# Covert the `commits` object into a tidy data frame 
(commits_df = unnest_wider(commits, steps) %>%
  select(commit, files) %>%
  unnest_wider(commit, names_repair = "unique") %>%
  unnest_wider(author, names_repair = "unique") %>%
  select(name, date, files) %>%
  mutate(name = replace(name, name == "Romain François", "Romain Francois"))%>%
  mutate(name = replace(name, name == "DavisVaughan", "Davis Vaughan")) %>%
  mutate(name = replace(name, name == "Mine Çetinkaya-Rundel", "Mine Cetinkaya-Rundel")))

# The thought that guides our process of tidying the dataset is try to select as few columns as we can, unnest as much as we can and prepare a clean dataset for Part 2.
# Note here we are only interested in "commit" and "files" columns for Part 2, so we delete other columns to tidy the data. Also, we keep "files" as a list of lists, so we only unnest "commit" column and "author" column inside the "commit" column.
# Also, we replace those important non-English names into English to clean the data.
```

```{r task2_top}
# Print out the first 10 lines of your tidy data
commits_df %>% slice_head(n = 10)
```


### Part 3 - Questions


#### Question 1

```{r task2_q1}
# Here we use the "name" element inside "author" element, which is inside the "commit" element.
commits_df %>%
  select(name) %>%
  filter(is.na(name) == FALSE) %>%
  group_by(name) %>%
  summarise(n = n(), .groups = 'drop') %>%
  top_n(n = 5) %>%
  arrange(desc(n))
```

#### Question 2

```{r task2_q2}
# Employee is a tidied vector of the Rstudio employees' name.
employee = c(
"E. David Aja",
"Adam Conroy",
"Alex Gold",
"Alison Hill",
"Amanda Gadrow",
"Andrie de Vries",
"Andy Kipp",
"Ann Vermeersch",
"Anne Romano",
"Aron Atkins",
"Ashley Henry",
"Barret Schloerke",
"Ben Kietzman",
"Bill Sager",
"Brian Law",
"Brian Smith",
"Carl Howe",
"Carson Sievert",
"Chaita Chaudhari",
"Charles Teague",
"Chris Tierney",
"Christina Medeiros",
"Clay Walker",
"Cole Arendt",
"Curtis Kephart",
"Dan Buch",
"Daniel Falbel",
"Daniel Petzold",
"Daniel Rodriguez",
"Darby Hadley",
"Dave Hurst",
"DavisVaughan",
"Derrick Kearney",
"Desirée De Leon",
"Devin Johnson",
"Donna Choi",
"Elena Ruiz",
"Elisa Gladu",
"Emily Crisan",
"Eric Pite",
"Ezgi Karaesmen",
"Ferit Albukrek",
"Francois Saint-Jacques",
"Gabor Csardi",
"Gagandeep Singh",
"Garrett Grolemund",
"Gary Ritchie",
"Greg Lin",
"Greg Swinehart",
"Greg Wilson",
"Hadley Wickham",
"Hadrien Dykiel",
"Heath Young",
"Ian Flores Siaca",
"Ian Pylvainen",
"Ingrid Rodriguez",
"J.J. Allaire",
"James Blair",
"Janeka Handford",
"Jason Milnes",
"Javier Luraschi",
"Jay Clark",
"Jeff Allen",
"Jen Hecht",
"Jenn Allen",
"Jenny Bryan",
"Jeremy Lang",
"Jessica Brennan",
"Jill Solovey",
"Jim Clemens",
"Jim Hester",
"Joe Cheng",
"Jonathan Curran",
"Jonathan McPherson",
"Jonathan Yoder",
"JooYoung Seo",
"Joseph Rickert",
"Josh Forest",
"Josh Spiewak",
"Josiah Parry",
"Julia Silge",
"Kaia Cooper",
"Kaitlyn Horwitz",
"Karen Medina",
"Karl Feinauer",
"Katherine Vu",
"Katie Masiello",
"Katie Shellenberger",
"Kayla Laenen",
"Kelly O'Briant",
"Kevin Gartland",
"Kevin Hankens",
"Kevin Hayden",
"Kevin Kuo",
"Kevin Ushey",
"Kristopher Overholt",
"Latoya Rutherford-Littlejohn",
"Lauren Chadwick",
"Lawrence Mayfield",
"Leticia Lima",
"Lionel Henry",
"Lou Bajuk",
"Madhulika Tanuboddi",
"Mara Averick",
"Maria Semple",
"Mario Ferrini",
"Mark Engeln",
"Matt Lands",
"Matt Quarles",
"Max Kuhn",
"Mel Gregory",
"Melissa Barca",
"Michael Demsko Jr.",
"Michael Marchetti",
"Michael Sarahan",
"Mike Bessuille",
"Mine Cetinkaya-Rundel",
"Ming Beckwith",
"Monte Jones",
"Nathan Calies",
"Nathan Stephens",
"Neal Richardson",
"Neha Bawa",
"Nichole Monhait",
"Nick Rohrbaugh",
"Nischal Shrestha",
"Omar Baba",
"Paulina Staszuk",
"Pete Knast",
"Phil Bowsher",
"Rachael Dempsey",
"Ralf Stubner",
"Ricardo Andrade",
"Rich Iannone",
"Richard Chen",
"Richard McCombie",
"Rick Johnson",
"Rob Hewardt",
"Robby Shaver",
"Robert Bethell",
"Roger Oberg",
"Romain Francois",
"Ron Blum",
"Ryan Johnson",
"Sabrina Evans",
"Sam Perman",
"Samantha Toet",
"Sandra Wilkins",
"Sarah Lin",
"Saudia Ganie",
"Sean Lopp",
"Sean Sinnott",
"Shalu Tiwari",
"Shannon Hagerty",
"Shawn Burke",
"Sigrid Keydana",
"Simon Couch",
"Stephen Kress",
"Stephen Siegert",
"Steve Nolen",
"Steve Wan",
"Swati Sinha",
"Taner Alkaya",
"Tareef Kawaf",
"Taylor Hoover",
"Thomas Pedersen",
"Tim McDuff",
"Tom Mock",
"Toni Noble",
"Tonya Filz",
"Tyler Finethy",
"Wes McKinney",
"Winston Chang",
"Yihui Xie",
"Yitao Li",
"Davis Vaughan"
)

commits_df %>%
  select(name) %>%
  filter(is.na(name) == FALSE) %>%
  group_by(name) %>%
  summarise(n = n(), .groups = 'drop') %>%
  filter(! name %in% employee) %>%
  top_n(n = 1) %>%
  arrange(desc(n))

# Kirill Müller is the top contributor who does not work for Rstudio now. He modified NAMESPACE on 2020-04-30.
```


#### Question 3

```{r task2_q3}
commits_df %>%
  unnest_longer(files) %>%
  unnest_wider(files) %>%
  select(filename) %>%
  filter(is.na(filename) == FALSE) %>%
  group_by(filename) %>%
  summarise(n = n(), .groups = 'drop') %>%
  top_n(n = 4) %>%
  arrange(desc(n))
```

#### Question 4

```{r task2_q4}
commits_time = commits_df %>%
  select(date) %>%
  filter(is.na(date) == FALSE) %>%
  mutate(hour = substr(date, 12, 13)) %>%
  mutate(day = weekdays(as.Date(substr(date, 1, 10)))) %>%
  mutate(day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>%
  select(day, hour)

ggplot(commits_time, aes(x = hour, )) +
  geom_bar(fill = "deepskyblue3") +
  labs(title = "Number of commits for each hour of the day", x = "Hours", y = "Number of Commits") +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(commits_time, aes(x = day, )) +
  geom_bar(fill = "green4") +
  labs(title = "Number of commits for each day of the week", x = "Days", y = "Number of Commits") +
  theme(plot.title = element_text(hjust = 0.5))

table(commits_time)

# From the graph we know that most of the commits are made during the working days.
# Most commits are made from 12am to 3pm and 8pm to 11pm.
```

#### Question 5

```{r task2_q5}
commits_date = commits_df %>%
  select(date) %>%
  filter(is.na(date) == FALSE) %>%
  mutate(date = as.Date(substr(date, 1, 10))) %>%
  select(date) %>%
  group_by(date) %>%
  summarise(count = n(), .groups = "drop")

update_time = as.Date(c("2020-01-31", "2020-03-07", "2020-05-29", "2020-07-31", "2020-08-12"))

ggplot(commits_date, aes(x = date, y = count)) +
  geom_line(color = "green4", size = 1) +
  geom_vline(xintercept = update_time, color = "red2") +
  labs(title = "The commit history", x = "Date", y = "Number of Commits") +
  theme(plot.title = element_text(hjust = 0.5))

# From the plot, we can find several interesting patterns:
# Firstly, the number of commits is relatively higher around or before the time that the package was updated.
# Secondly, the number of commits has the decreasing trend as time passing by.
# Thirdly, the number of commits reaches super high level from about January to February, which is before the first update of this year.
```
